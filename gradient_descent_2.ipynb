{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84835c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from time import time\n",
    "from itertools import product\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca616db",
   "metadata": {},
   "source": [
    "# Regular Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab2852c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(point, gamma, gradient, n_iter=1, epsilon=0.001):\n",
    "    p = point\n",
    "    points = []\n",
    "    for i in range(n_iter):\n",
    "        p = p - gamma * gradient(*p)\n",
    "        points.append(p)\n",
    "        if i != 0 and np.linalg.norm(p - points[-2]) < epsilon:\n",
    "            print(f\"Gradient descent converged in {i} steps!\")\n",
    "            return p, points\n",
    "    return p, points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b966800",
   "metadata": {},
   "outputs": [],
   "source": [
    "def timed_gradient_descent(point, gamma, gradient, time_limit=0.1, epsilon=0.001):\n",
    "    p = point\n",
    "    points = []\n",
    "    start, now = time(), time()\n",
    "    while now-start < time_limit:\n",
    "        p = p - gamma * gradient(*p)\n",
    "        points.append(p)\n",
    "        if len(points) >=2 and np.linalg.norm(p - points[-2]) < epsilon:\n",
    "            print(f\"Gradient descent converged in {now - start} seconds!\")\n",
    "            return p, points\n",
    "        now = time()\n",
    "    return p, points"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1269e112",
   "metadata": {},
   "source": [
    "# Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ebc9010",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stochastic_gradient_descent(point, gamma, gradient, n_iter=1, epsilon=0.001):\n",
    "    p = point\n",
    "    points = []\n",
    "    for i in range(n_iter):\n",
    "        p = p - gamma * gradient(*p)\n",
    "        points.append(p)\n",
    "        if i != 0 and np.linalg.norm(p - points[-2]) < epsilon:\n",
    "            print(f\"Stochastic gradient descent converged in {i} steps!\")\n",
    "            return p, points\n",
    "    return p, points"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d55a5ae",
   "metadata": {},
   "source": [
    "# Polyak Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f4a6402",
   "metadata": {},
   "outputs": [],
   "source": [
    "def polyak_gradient_descent(point, gamma, mu, gradient, n_iter=1, epsilon=0.001):\n",
    "    p = point\n",
    "    points = []\n",
    "    for i in range(n_iter):\n",
    "        if len(points) > 2:\n",
    "            p = p - gamma * gradient(*p) + mu * (p - points[-2])\n",
    "        else:\n",
    "            p = p - gamma * gradient(*p)\n",
    "\n",
    "        points.append(p)\n",
    "\n",
    "        if i != 0 and np.linalg.norm(p - points[-2]) < epsilon:\n",
    "            print(f\"Polyak gradient descent converged in {i} seconds!\")\n",
    "            return p, points\n",
    "    return p, points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f3e39a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def timed_polyak_gradient_descent(point, gamma, mu, gradient, time_limit=0.1, epsilon=0.001):\n",
    "    p = point\n",
    "    points = []\n",
    "    start, now = time(), time()\n",
    "    while now-start < time_limit:\n",
    "        if len(points) > 2:\n",
    "            p = p - gamma * gradient(*p) + mu * (p - points[-2])\n",
    "        else:\n",
    "            p = p - gamma * gradient(*p)\n",
    "\n",
    "        points.append(p)\n",
    "\n",
    "        if len(points) > 2 and np.linalg.norm(p - points[-2]) < epsilon:\n",
    "            print(f\"Polyak gradient descent converged in {now - start} steps!\")\n",
    "            return p, points\n",
    "        now = time()\n",
    "    return p, points"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a6339f7",
   "metadata": {},
   "source": [
    "# Nesterov Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "53df4a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nesterov_gradient_descent(point, gamma, mu, gradient, n_iter=1, epsilon=0.001):\n",
    "    p = point\n",
    "    points = []\n",
    "    for i in range(n_iter):\n",
    "        if len(points) > 2:\n",
    "            p = p - gamma * gradient(*(p + mu * (p - points[-2]))) + mu * (p - points[-2])\n",
    "        else:\n",
    "            p = p - gamma * gradient(*p)\n",
    "        \n",
    "        points.append(p)\n",
    "        \n",
    "        if i != 0 and np.linalg.norm(p - points[-2]) < epsilon:\n",
    "            print(f\"Nesterov gradient descent converged in {i} steps!\")\n",
    "            return p, points\n",
    "    return p, points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "60abc404",
   "metadata": {},
   "outputs": [],
   "source": [
    "def timed_nesterov_gradient_descent(point, gamma, mu, gradient, time_limit=0.1, epsilon=0.001):\n",
    "    p = point\n",
    "    points = []\n",
    "    start, now = time(), time()\n",
    "    while now-start < time_limit:\n",
    "        if len(points) > 2:\n",
    "            p = p - gamma * gradient(*(p + mu * (p - points[-2]))) + mu * (p - points[-2])\n",
    "        else:\n",
    "            p = p - gamma * gradient(*p)\n",
    "        \n",
    "        points.append(p)\n",
    "        \n",
    "        if len(points) > 2 and np.linalg.norm(p - points[-2]) < epsilon:\n",
    "            print(f\"Nesterov gradient descent converged in {now - start} seconds!\")\n",
    "            return p, points\n",
    "        now = time()\n",
    "    return p, points"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b5a600c",
   "metadata": {},
   "source": [
    "# AdaGrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "44c4c4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adaGrad(point, gamma, gradient, n_iter=1, epsilon=0.001):\n",
    "    p = point\n",
    "    points = []\n",
    "    gradients = np.zeros((n_iter, point.shape[0]))\n",
    "    for i in range(n_iter):\n",
    "        grad = gradient(*p)\n",
    "        squared_gradient = np.square(grad)\n",
    "        gradients[i] = squared_gradient\n",
    "        d = np.sum(gradients, axis=0)\n",
    "        sqrt_d = np.sqrt(d)\n",
    "        div_sqrd_d = 1/(sqrt_d + 1e-7)\n",
    "        D = np.diag(div_sqrd_d)\n",
    "        p = p - gamma * (D @ grad)\n",
    "        points.append(p)\n",
    "\n",
    "        if i != 0 and np.linalg.norm(p - points[-2]) < epsilon:\n",
    "            print(f\"AdaGrad converged in {i} steps!\")\n",
    "            return p, points\n",
    "    return p, points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d86494f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def timed_adaGrad(point, gamma, gradient, time_limit=0.1, epsilon=0.001):\n",
    "    p = point\n",
    "    points = []\n",
    "    gradients = np.zeros(point.shape[0])\n",
    "    start, now = time(), time()\n",
    "    while now-start < time_limit:\n",
    "        grad = gradient(*p)\n",
    "        squared_gradient = np.square(grad)\n",
    "        gradients += squared_gradient\n",
    "        sqrt_d = np.sqrt(gradients)\n",
    "        div_sqrd_d = 1/(sqrt_d + 1e-7)\n",
    "        D = np.diag(div_sqrd_d)\n",
    "        p = p - gamma * (D @ grad)\n",
    "        points.append(p)\n",
    "\n",
    "        if len(points) > 2 and np.linalg.norm(p - points[-2]) < epsilon:\n",
    "            print(f\"AdaGrad converged in {now-start} seconds!\")\n",
    "            return p, points\n",
    "        now = time()\n",
    "    return p, points"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc817fb7",
   "metadata": {},
   "source": [
    "# Newton method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a5f115bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def newton_method(point, gradient, hessian, n_iter=1, epsilon=0.001):\n",
    "    p = point\n",
    "    points = []\n",
    "    for i in range(n_iter):\n",
    "        p = p - np.linalg.inv(hessian(*p)) @ gradient(*p)\n",
    "\n",
    "        points.append(p)\n",
    "        if i != 0 and np.linalg.norm(p - points[-2]) < epsilon:\n",
    "            print(f\"Newton method converged in {i} steps!\")\n",
    "            return p, points\n",
    "    return p, points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9b3b69cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def timed_newton_method(point, gradient, hessian, time_limit=0.1, epsilon=0.001):\n",
    "    p = point\n",
    "    points = []\n",
    "    start, now = time(), time()\n",
    "    while now-start < time_limit:\n",
    "        p = p - np.linalg.inv(hessian(*p)) @ gradient(*p)\n",
    "\n",
    "        points.append(p)\n",
    "        if len(points) > 2 and np.linalg.norm(p - points[-2]) < epsilon:\n",
    "            print(f\"Newton method converged in {now - start} seconds!\")\n",
    "            return p, points\n",
    "    return p, points"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e8bad23",
   "metadata": {},
   "source": [
    "# BFGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bd7cde8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def BFGS(point, gradient, n_iter=1, epsilon=0.001):\n",
    "    p = point\n",
    "    points = []\n",
    "    points.append(p)\n",
    "    H_approx = np.identity(p.shape[0])*0.001\n",
    "    previous_gradient = gradient(*p)\n",
    "    for i in range(n_iter):\n",
    "        \n",
    "        p = p - H_approx @ previous_gradient\n",
    "\n",
    "        current_gradient = gradient(*p)\n",
    "        delta_k = p - points[-1]\n",
    "        gamma_k = current_gradient - previous_gradient\n",
    "\n",
    "        points.append(p)\n",
    "\n",
    "        if i != 0 and np.linalg.norm(p - points[-2]) < epsilon:\n",
    "            print(f\"BFGS converged in {i} steps!\")\n",
    "            return p, points\n",
    "\n",
    "        nom1 = (np.outer(delta_k, gamma_k) @ H_approx) + (H_approx @ np.outer(gamma_k, delta_k))\n",
    "        denom1 = np.inner(delta_k, gamma_k)\n",
    "\n",
    "        nom2 = np.inner(gamma_k, H_approx@gamma_k)\n",
    "        denom2 = np.inner(delta_k, gamma_k)\n",
    "\n",
    "        nom3 = np.outer(delta_k, delta_k)\n",
    "        denom3 = np.inner(delta_k, gamma_k)\n",
    "\n",
    "        H_approx = H_approx - (nom1/denom1) + ((1 + (nom2/denom2)) * (nom3/denom3))\n",
    "\n",
    "        previous_gradient = current_gradient\n",
    "    return p, points\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "83f9dd91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def timed_BFGS(point, gradient, time_limit=0.1, epsilon=0.001):\n",
    "    p = point\n",
    "    points = []\n",
    "    points.append(p)\n",
    "    H_approx = np.identity(p.shape[0]) * 0.001\n",
    "    previous_gradient = gradient(*p)\n",
    "    start, now = time(), time()\n",
    "    while now-start < time_limit:\n",
    "        \n",
    "        p = p - H_approx @ previous_gradient\n",
    "\n",
    "        current_gradient = gradient(*p)\n",
    "        delta_k = p - points[-1]\n",
    "        gamma_k = current_gradient - previous_gradient\n",
    "\n",
    "        points.append(p)\n",
    "\n",
    "        if len(points) > 2 and np.linalg.norm(p - points[-2]) < epsilon:\n",
    "            print(f\"BFGS converged in {now - start} seconds!\")\n",
    "            return p, points\n",
    "\n",
    "        nom1 = (np.outer(delta_k, gamma_k) @ H_approx) + (H_approx @ np.outer(gamma_k, delta_k))\n",
    "        denom1 = np.inner(delta_k, gamma_k)\n",
    "\n",
    "        nom2 = np.inner(gamma_k, H_approx@gamma_k)\n",
    "        denom2 = np.inner(delta_k, gamma_k)\n",
    "\n",
    "        nom3 = np.outer(delta_k, delta_k)\n",
    "        denom3 = np.inner(delta_k, gamma_k)\n",
    "\n",
    "        H_approx = H_approx - (nom1/denom1) + ((1 + (nom2/denom2)) * (nom3/denom3))\n",
    "\n",
    "        previous_gradient = current_gradient\n",
    "        now = time()\n",
    "    return p, points"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb43c4b",
   "metadata": {},
   "source": [
    "# L-BFGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8d4ab1eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_BFGS(point, gradient, m, n_iter=1, epsilon=0.001):\n",
    "    p = point\n",
    "    points = []\n",
    "    gradients = []\n",
    "    points.append(p)\n",
    "\n",
    "    for k in range(n_iter):\n",
    "        q = gradient(*p)\n",
    "        gradients.append(q)\n",
    "\n",
    "        alphas = []\n",
    "        j = min(len(gradients), m)\n",
    "        for i in range(len(gradients)-1, len(gradients)- j, -1):\n",
    "            delta_i = points[i] - points[i-1]\n",
    "            gamma_i = gradients[i] - gradients[i-1]\n",
    "            rho_i = 1/np.dot(delta_i, gamma_i)\n",
    "            a_i = rho_i*np.dot(delta_i, q)\n",
    "            alphas.append(a_i)\n",
    "            q = q - a_i*gamma_i\n",
    "\n",
    "        if len(gradients) == 1:\n",
    "            Bk = np.identity(point.shape[0])\n",
    "        else:\n",
    "            delta_i = points[-1] - points[-2]\n",
    "            gamma_i = gradients[-1] - gradients[-2]\n",
    "            Bk = ((np.dot(delta_i, gamma_i))/np.dot(gamma_i, gamma_i)) * np.identity(point.shape[0])\n",
    "        r = Bk @ q\n",
    "\n",
    "        for off, i in enumerate(range(len(gradients)- j + 1, len(gradients))):\n",
    "            delta_i = points[i] - points[i-1]\n",
    "            gamma_i = gradients[i] - gradients[i-1]\n",
    "            rho_i = 1/np.dot(delta_i, gamma_i)\n",
    "            a_i = alphas[-(off+1)]\n",
    "            beta = rho_i * np.dot(gamma_i, r)\n",
    "            r = r + delta_i*(a_i - beta)\n",
    "        p = p - r\n",
    "        points.append(p)\n",
    "\n",
    "        if k != 0 and np.linalg.norm(points[-1] - points[-2]) < epsilon:\n",
    "            print(f\"L-FBGS converged in {k} steps\")\n",
    "            return p, points\n",
    "    return p, points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7bef1ec3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "3\n",
      "2\n",
      "1\n",
      "_\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "a = [1,2,3,4,5]\n",
    "m = 8\n",
    "j = min(len(a), m)\n",
    "for i in range(len(a)-1, len(a)- j, -1):\n",
    "    print(i)\n",
    "print(\"_\")\n",
    "for i in range(len(a)- j + 1, len(a)):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c4bf6732",
   "metadata": {},
   "outputs": [],
   "source": [
    "def func(x, y):\n",
    "    return x**2 + np.e**x + y**2 -x*y\n",
    "\n",
    "def grad_func(x, y):\n",
    "    return np.array([2*x + np.e**x - y, 2*y -x])\n",
    "\n",
    "def hessian_func(x, y):\n",
    "    return np.array([[2 + np.e**x, -1],\n",
    "                     [-1, 2]])"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "6a156873",
   "metadata": {},
   "source": [
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8e971cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1(x, y, z):\n",
    "    return (x - z) ** 2 + (2*y + z) ** 2 + (4*x -2*y + z) ** 2 + x + y\n",
    "\n",
    "def grad_f1(x, y, z):\n",
    "    dx = 34*x - 16*y + 6*z + 1\n",
    "    dy = -16*x + 16*y + 1\n",
    "    dz = 6*x + 6*z\n",
    "    return np.array([dx, dy, dz])\n",
    "\n",
    "def hessian_f1(x, y, z):\n",
    "    return np.array([[34, -16, 6],\n",
    "                [-16, 16, 0],\n",
    "                [6, 0, 6]])\n",
    "\n",
    "def f2(x, y, z):\n",
    "    return ((x - 1) ** 2) + ((y - 1) ** 2) + 100*((y - (x ** 2)) ** 2) + 100*((z - (y ** 2)) ** 2)\n",
    "\n",
    "def grad_f2(x, y , z):\n",
    "    dx = 400 * (x ** 3) - 400*x*y +2*x -2\n",
    "    dy = 400*(y ** 3) - 200*(x ** 2) - 400*z*y + 202*y -2\n",
    "    dz = 200*z - 200*(y ** 2)\n",
    "    return np.array([dx, dy, dz])\n",
    "\n",
    "def hessian_f2(x, y, z):\n",
    "    return np.array([[1200 * x ** 2 - 400 * y + 2, -400 * x, 0],\n",
    "              [-400 * x, 1200 * y ** 2 - 400 * z + 202, -400 * y],\n",
    "              [0, -400 * y, 200]])\n",
    "\n",
    "def f3(x, y):\n",
    "    return (1.5 - x + x*y) ** 2 + (2.25 - x + x*(y ** 2)) ** 2 + (2.625 - x + x*(y ** 3)) ** 2\n",
    "\n",
    "def grad_f3(x, y):\n",
    "    dx = 2*(1.5 - x + x*y)*(y-1) + 2*(2.25 - x + x*(y ** 2))*(y ** 2 - 1) + 2*(2.625 - x + x*(y ** 3))*(y ** 3 -1)\n",
    "    dy = 2*x*(1.5 - x + x*y) + 4*x*y*(2.25 - x + x*(y ** 2)) + 6*x*(y ** 2)*(2.625 - x + x*(y ** 3))\n",
    "    return np.array([dx, dy])\n",
    "\n",
    "def hessian_f3(x, y):\n",
    "    return np.array([[2*(y-1)**2 + 2*(y**2-1)**2 + 2*(y**3-1)**2, 2*(1.5-x+x*y) + 2*x*(y-1) +4*y*(2.25-x+x*y**2) + 4*x*y*(y**2-1) +6*y**2*(2.625-x+x*y**3)+6*x*y**2*(y**3-1)],\n",
    "                     [2*(1.5-x+x*y) + 2*x*(y-1) +4*y*(2.25-x+x*y**2) + 4*x*y*(y**2-1) +6*y**2*(2.625-x+x*y**3)+6*x*y**2*(y**3-1), 2*x**2+4*x*(2.25-x+x*y**2)+8*x**2*y**2+12*x*y*(2.625-x+x*y**3)+18*x**2*y**4]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3f704382",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_methods_with_steps(starting_point, function, gradient, hessian, n_iter, epsilon, lr):\n",
    "\n",
    "    point1, points1 = gradient_descent(starting_point, lr, gradient, n_iter, epsilon)\n",
    "    print(f\"Gradient Descent: x = {point1}  |  f(x) = {function(*point1)}\")\n",
    "\n",
    "    point2, points2 = polyak_gradient_descent(starting_point, lr, 0.5, gradient, n_iter, epsilon)\n",
    "    print(f\"Polyak Gradient Descent: x = {point2}  |  f(x) = {function(*point2)}\")\n",
    "\n",
    "    point3, points3 = nesterov_gradient_descent(starting_point, lr, 0.5, gradient, n_iter, epsilon)\n",
    "    print(f\"Nesterov Gradient Descent: x = {point3}  |  f(x) = {function(*point3)}\")\n",
    "\n",
    "    point4, points4 = adaGrad(starting_point, 1, gradient, n_iter, epsilon)\n",
    "    print(f\"AdaGrad Gradient Descent: x = {point4}  |  f(x) = {function(*point4)}\")\n",
    "\n",
    "    point5, points5 = newton_method(starting_point, gradient, hessian, n_iter)\n",
    "    print(f\"Newton method: x = {point5}  |  f(x) = {function(*point5)}\")\n",
    "\n",
    "    point6, points6 = BFGS(starting_point, gradient, n_iter, epsilon)\n",
    "    print(f\"BFGS: x = {point6}  |  f(x) = {function(*point6)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "de093526",
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = [2,5,10,100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d6456a85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting point: [0 0 0], number of steps: 2\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Gradient Descent: x = [-0.0182 -0.02    0.0006]  |  f(x) = -0.03525736\n",
      "Polyak Gradient Descent: x = [-0.0182 -0.02    0.0006]  |  f(x) = -0.03525736\n",
      "Nesterov Gradient Descent: x = [-0.0182 -0.02    0.0006]  |  f(x) = -0.03525736\n",
      "AdaGrad Gradient Descent: x = [-0.00172553 -1.70710663  0.99999998]  |  f(x) = 24.547438853812828\n",
      "Newton method converged in 1 steps!\n",
      "Newton method: x = [-0.16666667 -0.22916667  0.16666667]  |  f(x) = -0.19791666666666663\n",
      "BFGS: x = [-0.11133333 -0.11333333  0.00066667]  |  f(x) = -0.11352266666666666\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Starting point: [1 1 0], number of steps: 2\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Gradient Descent: x = [ 0.6866  0.9512 -0.105 ]  |  f(x) = 6.041198320000001\n",
      "Polyak Gradient Descent: x = [ 0.6866  0.9512 -0.105 ]  |  f(x) = 6.041198320000001\n",
      "Nesterov Gradient Descent: x = [ 0.6866  0.9512 -0.105 ]  |  f(x) = 6.041198320000001\n",
      "AdaGrad Gradient Descent: x = [ 0.25449337 -0.70710717 -0.29289322]  |  f(x) = 7.337815769467778\n",
      "Newton method converged in 1 steps!\n",
      "Newton method: x = [-0.16666667 -0.22916667  0.16666667]  |  f(x) = -0.19791666666666669\n",
      "BFGS: x = [ 0.4263623   0.96011648 -0.18295831]  |  f(x) = 4.934072309630413\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Starting point: [0 0 0], number of steps: 5\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Gradient Descent: x = [-0.03722513 -0.04780083  0.00477825]  |  f(x) = -0.07265853373031797\n",
      "Polyak Gradient Descent: x = [-0.04525609 -0.05951459  0.00645777]  |  f(x) = -0.08633962950250537\n",
      "Nesterov Gradient Descent: x = [-0.04436033 -0.05880467  0.00691165]  |  f(x) = -0.08548164878067167\n",
      "AdaGrad Gradient Descent: x = [-0.54542549 -0.68553303  0.56789228]  |  f(x) = 0.7125305067070347\n",
      "Newton method converged in 1 steps!\n",
      "Newton method: x = [-0.16666667 -0.22916667  0.16666667]  |  f(x) = -0.19791666666666663\n",
      "BFGS: x = [-0.12236805 -0.19682176  0.02671127]  |  f(x) = -0.15754867159032315\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Starting point: [1 1 0], number of steps: 5\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Gradient Descent: x = [ 0.49060337  0.78298788 -0.18990303]  |  f(x) = 3.672912916835993\n",
      "Polyak Gradient Descent: x = [ 0.40701705  0.71234932 -0.22633215]  |  f(x) = 2.9571070832169757\n",
      "Nesterov Gradient Descent: x = [ 0.42973297  0.71008593 -0.21778043]  |  f(x) = 3.0113953469093895\n",
      "AdaGrad Gradient Descent: x = [-0.20934772 -0.01551997 -0.022861  ]  |  f(x) = 0.5004073411875176\n",
      "Newton method converged in 1 steps!\n",
      "Newton method: x = [-0.16666667 -0.22916667  0.16666667]  |  f(x) = -0.19791666666666669\n",
      "BFGS: x = [ 0.06067771  0.0742386  -0.45423277]  |  f(x) = 0.6231349569454796\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Starting point: [0 0 0], number of steps: 10\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Gradient Descent: x = [-0.06058477 -0.08473229  0.01620659]  |  f(x) = -0.11272088687798694\n",
      "Polyak Gradient Descent: x = [-0.08553583 -0.12444107  0.03173615]  |  f(x) = -0.14528647528011582\n",
      "Nesterov Gradient Descent: x = [-0.0837101  -0.12074968  0.03248679]  |  f(x) = -0.14356856715127075\n",
      "AdaGrad Gradient Descent: x = [-0.33975839 -0.45392085  0.38554072]  |  f(x) = 0.00948794644179346\n",
      "Newton method converged in 1 steps!\n",
      "Newton method: x = [-0.16666667 -0.22916667  0.16666667]  |  f(x) = -0.19791666666666663\n",
      "BFGS: x = [-0.11956279 -0.19874166  0.11385125]  |  f(x) = -0.18228066976109086\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Starting point: [1 1 0], number of steps: 10\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Gradient Descent: x = [ 0.33248914  0.5289074  -0.25006252]  |  f(x) = 1.8537141911193322\n",
      "Polyak Gradient Descent: x = [ 0.18771993  0.25307508 -0.29106194]  |  f(x) = 0.7184367145201136\n",
      "Nesterov Gradient Descent: x = [ 0.19706217  0.28661702 -0.27485307]  |  f(x) = 0.7989950201298206\n",
      "AdaGrad Gradient Descent: x = [-0.13176967 -0.19203482  0.12578264]  |  f(x) = -0.19046237329154356\n",
      "Newton method converged in 1 steps!\n",
      "Newton method: x = [-0.16666667 -0.22916667  0.16666667]  |  f(x) = -0.19791666666666669\n",
      "BFGS: x = [ 0.10834477  0.00312151 -0.21969743]  |  f(x) = 0.30767153005528486\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Starting point: [0 0 0], number of steps: 100\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Gradient Descent: x = [-0.16008983 -0.22113633  0.15398412]  |  f(x) = -0.1975283975531976\n",
      "Polyak Gradient Descent: x = [-0.16642652 -0.22887355  0.16620318]  |  f(x) = -0.19791614854588013\n",
      "Nesterov Gradient Descent: x = [-0.16636936 -0.22880379  0.16609287]  |  f(x) = -0.19791587259035154\n",
      "AdaGrad converged in 69 steps!\n",
      "AdaGrad Gradient Descent: x = [-0.16669602 -0.22920476  0.16670381]  |  f(x) = -0.19791666070317132\n",
      "Newton method converged in 1 steps!\n",
      "Newton method: x = [-0.16666667 -0.22916667  0.16666667]  |  f(x) = -0.19791666666666663\n",
      "BFGS converged in 16 steps!\n",
      "BFGS: x = [-0.16666667 -0.22916667  0.16666666]  |  f(x) = -0.1979166666666666\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Starting point: [1 1 0], number of steps: 100\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Gradient Descent: x = [-0.1410581  -0.19788032  0.11734984]  |  f(x) = -0.1920377316079215\n",
      "Polyak Gradient Descent: x = [-0.16573219 -0.22802609  0.16486314]  |  f(x) = -0.19790882163611745\n",
      "Nesterov Gradient Descent: x = [-0.1655098  -0.22775465  0.16443392]  |  f(x) = -0.19790464330625998\n",
      "AdaGrad converged in 52 steps!\n",
      "AdaGrad Gradient Descent: x = [-0.16664458 -0.22913887  0.16663703]  |  f(x) = -0.19791666330863808\n",
      "Newton method converged in 1 steps!\n",
      "Newton method: x = [-0.16666667 -0.22916667  0.16666667]  |  f(x) = -0.19791666666666669\n",
      "BFGS converged in 17 steps!\n",
      "BFGS: x = [-0.16666666 -0.22916666  0.16666665]  |  f(x) = -0.19791666666666627\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "starting_points1 = [np.array([0,0,0]), np.array([1,1,0])]\n",
    "\n",
    "for step, starting_point in product(steps, starting_points1):\n",
    "\n",
    "    print(f\"Starting point: {starting_point}, number of steps: {step}\")\n",
    "    print(\"-\"*100)\n",
    "    compare_methods_with_steps(starting_point, f1, grad_f1, hessian_f1, step, 1e-5, 0.01)\n",
    "    print(\"-\"*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ab8b404f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting point: [1.2 1.2 1.2], number of steps: 2\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Gradient Descent: x = [1.06535251 1.12528986 1.25486595]  |  f(x) = 0.04237239125217664\n",
      "Polyak Gradient Descent: x = [1.06535251 1.12528986 1.25486595]  |  f(x) = 0.04237239125217664\n",
      "Nesterov Gradient Descent: x = [1.06535251 1.12528986 1.25486595]  |  f(x) = 0.04237239125217664\n",
      "AdaGrad Gradient Descent: x = [0.32361212 1.10337583 1.20611626]  |  f(x) = 100.21139255011029\n",
      "Newton method: x = [1.05776971 1.11424466 1.22854767]  |  f(x) = 0.03541789639142505\n",
      "BFGS: x = [1.06192568 1.12399037 1.25610646]  |  f(x) = 0.02582746997757629\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Starting point: [-1.   1.2  1.2], number of steps: 2\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Gradient Descent: x = [-1.02305097  1.13267024  1.21655427]  |  f(x) = 5.291303548299122\n",
      "Polyak Gradient Descent: x = [-1.02305097  1.13267024  1.21655427]  |  f(x) = 5.291303548299122\n",
      "Nesterov Gradient Descent: x = [-1.02305097  1.13267024  1.21655427]  |  f(x) = 5.291303548299122\n",
      "AdaGrad Gradient Descent: x = [-1.00031112  1.18641674  1.20611626]  |  f(x) = 11.546903892218364\n",
      "Newton method: x = [-0.89079472  0.74508573  0.32739814]  |  f(x) = 9.061843265953685\n",
      "BFGS: x = [-1.04291845  1.09951919  1.22835632]  |  f(x) = 4.235128970788185\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Starting point: [1.2 1.2 1.2], number of steps: 5\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Gradient Descent: x = [1.05903901 1.12114143 1.25762354]  |  f(x) = 0.01822295618259744\n",
      "Polyak Gradient Descent: x = [1.05609859 1.12022306 1.25847131]  |  f(x) = 0.021256581090361628\n",
      "Nesterov Gradient Descent: x = [1.05876619 1.12032348 1.25786774]  |  f(x) = 0.018727509319134247\n",
      "AdaGrad Gradient Descent: x = [0.35399251 0.3857708  0.9171115 ]  |  f(x) = 66.6058692559764\n",
      "Newton method: x = [1.00284029 1.00568037 1.01133575]  |  f(x) = 4.066868659391707e-05\n",
      "BFGS: x = [1.05876107 1.12120438 1.2576074 ]  |  f(x) = 0.018174446933573438\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Starting point: [-1.   1.2  1.2], number of steps: 5\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Gradient Descent: x = [-1.04880797  1.09746989  1.22320784]  |  f(x) = 4.242976298680137\n",
      "Polyak Gradient Descent: x = [-1.04097731  1.11291771  1.21613246]  |  f(x) = 4.314509070716151\n",
      "Nesterov Gradient Descent: x = [-1.05792414  1.07545624  1.22729706]  |  f(x) = 4.931848380209836\n",
      "AdaGrad Gradient Descent: x = [-1.03287018  1.07724836  1.24385303]  |  f(x) = 4.844774474078231\n",
      "Newton method: x = [-0.17195775 -0.09897162  0.00457768]  |  f(x) = 4.236227155367737\n",
      "BFGS: x = [-1.04668471  1.10385317  1.22143011]  |  f(x) = 4.207463258956974\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Starting point: [1.2 1.2 1.2], number of steps: 10\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Gradient Descent: x = [1.05864752 1.12098622 1.25711121]  |  f(x) = 0.018108640302897747\n",
      "Polyak Gradient Descent: x = [1.05846787 1.12068066 1.25646739]  |  f(x) = 0.01802237304149247\n",
      "Nesterov Gradient Descent: x = [1.05766037 1.12208871 1.25600143]  |  f(x) = 0.020365610338217165\n",
      "AdaGrad Gradient Descent: x = [0.75400841 0.57547402 0.3461881 ]  |  f(x) = 0.2681112473921844\n",
      "Newton method converged in 6 steps!\n",
      "Newton method: x = [1.00000014 1.00000028 1.00000056]  |  f(x) = 9.882821549315355e-14\n",
      "BFGS: x = [1.04885491 1.09532804 1.20164432]  |  f(x) = 0.014109486459787688\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Starting point: [-1.   1.2  1.2], number of steps: 10\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Gradient Descent: x = [-1.04485612  1.10184568  1.21715482]  |  f(x) = 4.203008679052148\n",
      "Polyak Gradient Descent: x = [-1.04297411  1.10020955  1.21136904]  |  f(x) = 4.199279735525845\n",
      "Nesterov Gradient Descent: x = [-1.01389834  1.12307916  1.18918352]  |  f(x) = 5.49530943006301\n",
      "AdaGrad Gradient Descent: x = [-1.04002239  1.09694536  1.20098831]  |  f(x) = 4.195024416233707\n",
      "Newton method: x = [-2.53284987  5.77125422 30.02149851]  |  f(x) = 1156.427673877216\n",
      "BFGS: x = [-0.99502488  0.96881517  0.92033634]  |  f(x) = 4.059659226017497\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Starting point: [1.2 1.2 1.2], number of steps: 100\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Gradient Descent: x = [1.05666518 1.11675785 1.24766448]  |  f(x) = 0.016874692199929757\n",
      "Polyak Gradient Descent: x = [1.05459539 1.1123801  1.23788789]  |  f(x) = 0.01563913847363457\n",
      "Nesterov Gradient Descent: x = [1.01180089 1.02434045 1.04882082]  |  f(x) = 0.0007881257935985029\n",
      "AdaGrad Gradient Descent: x = [0.80290641 0.64391124 0.41257841]  |  f(x) = 0.1661184537953873\n",
      "Newton method converged in 6 steps!\n",
      "Newton method: x = [1.00000014 1.00000028 1.00000056]  |  f(x) = 9.882821549315355e-14\n",
      "BFGS converged in 19 steps!\n",
      "BFGS: x = [1. 1. 1.]  |  f(x) = 2.1701196816096958e-17\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Starting point: [-1.   1.2  1.2], number of steps: 100\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Gradient Descent: x = [-1.02945794  1.06921156  1.14709682]  |  f(x) = 4.133886443347678\n",
      "Polyak Gradient Descent: x = [-1.01194076  1.03350918  1.07199061]  |  f(x) = 4.0595069337017025\n",
      "Nesterov Gradient Descent: x = [-0.98399528  0.97782646  0.95993771]  |  f(x) = 3.9473448722423563\n",
      "AdaGrad Gradient Descent: x = [-1.01736164  1.0428302   1.09012029]  |  f(x) = 4.0783642898831785\n",
      "Newton method converged in 32 steps!\n",
      "Newton method: x = [1. 1. 1.]  |  f(x) = 2.9889555706090917e-19\n",
      "BFGS: x = [0.71743337 0.51552359 0.26882621]  |  f(x) = 0.3155647462576267\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "starting_points2 = [np.array([1.2, 1.2, 1.2]), np.array([-1, 1.2, 1.2])]\n",
    "\n",
    "for step, starting_point in product(steps, starting_points2):\n",
    "\n",
    "    print(f\"Starting point: {starting_point}, number of steps: {step}\")\n",
    "    print(\"-\"*100)\n",
    "    compare_methods_with_steps(starting_point, f2, grad_f2, hessian_f2, step, 1e-5, 0.001)\n",
    "    print(\"-\"*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6cce93d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting point: [1 1], number of steps: 2\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Gradient Descent: x = [1.00073394 0.94633927]  |  f(x) = 12.807584968367372\n",
      "Polyak Gradient Descent: x = [1.00073394 0.94633927]  |  f(x) = 12.807584968367372\n",
      "Nesterov Gradient Descent: x = [1.00073394 0.94633927]  |  f(x) = 12.807584968367372\n",
      "AdaGrad Gradient Descent: x = [ 1.99999999 -0.03601266]  |  f(x) = 0.7815248553242349\n",
      "Newton method converged in 1 steps!\n",
      "Newton method: x = [0. 1.]  |  f(x) = 14.203125\n",
      "BFGS: x = [1.01107332 0.57690351]  |  f(x) = 6.900699454310788\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Starting point: [4.5 4.5], number of steps: 2\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Gradient Descent: x = [1.93681506e+13 1.86674461e+13]  |  f(x) = 1.5873977012198753e+106\n",
      "Polyak Gradient Descent: x = [1.93681506e+13 1.86674461e+13]  |  f(x) = 1.5873977012198753e+106\n",
      "Nesterov Gradient Descent: x = [1.93681506e+13 1.86674461e+13]  |  f(x) = 1.5873977012198753e+106\n",
      "AdaGrad Gradient Descent: x = [3.32778606 3.32701033]  |  f(x) = 16211.099477320058\n",
      "Newton method: x = [3.28398585 3.2993568 ]  |  f(x) = 15043.650876647653\n",
      "BFGS: x = [103.804869   -98.53366445]  |  f(x) = 9862542002828528.0\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Starting point: [1 1], number of steps: 5\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Gradient Descent: x = [1.00633891 0.87741674]  |  f(x) = 11.251866331347744\n",
      "Polyak Gradient Descent: x = [1.0085334  0.84835485]  |  f(x) = 10.667805682601148\n",
      "Nesterov Gradient Descent: x = [1.00920561 0.85018343]  |  f(x) = 10.700778474905524\n",
      "AdaGrad Gradient Descent: x = [2.22639269 0.17246017]  |  f(x) = 0.2934484843668905\n",
      "Newton method converged in 1 steps!\n",
      "Newton method: x = [0. 1.]  |  f(x) = 14.203125\n",
      "BFGS: x = [ 1.0747495  -0.10202891]  |  f(x) = 3.906975264867373\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Starting point: [4.5 4.5], number of steps: 5\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Gradient Descent: x = [nan nan]  |  f(x) = nan\n",
      "Polyak Gradient Descent: x = [nan nan]  |  f(x) = nan\n",
      "Nesterov Gradient Descent: x = [nan nan]  |  f(x) = nan\n",
      "AdaGrad Gradient Descent: x = [3.03925375 3.03609447]  |  f(x) = 7964.859402353931\n",
      "Newton method: x = [1.95925605 2.07453011]  |  f(x) = 418.80402638225434\n",
      "BFGS: x = [nan nan]  |  f(x) = nan\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Starting point: [1 1], number of steps: 10\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Gradient Descent: x = [1.02327121 0.783514  ]  |  f(x) = 9.459676279367622\n",
      "Polyak Gradient Descent: x = [1.04520745 0.67738182]  |  f(x) = 7.81696049125177\n",
      "Nesterov Gradient Descent: x = [1.0476632  0.68458592]  |  f(x) = 7.89657739506707\n",
      "AdaGrad Gradient Descent: x = [2.47909975 0.32183467]  |  f(x) = 0.08584473847748889\n",
      "Newton method converged in 1 steps!\n",
      "Newton method: x = [0. 1.]  |  f(x) = 14.203125\n",
      "BFGS: x = [ 1.36751024 -0.49880175]  |  f(x) = 2.9804186498011482\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Starting point: [4.5 4.5], number of steps: 10\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Gradient Descent: x = [nan nan]  |  f(x) = nan\n",
      "Polyak Gradient Descent: x = [nan nan]  |  f(x) = nan\n",
      "Nesterov Gradient Descent: x = [nan nan]  |  f(x) = nan\n",
      "AdaGrad Gradient Descent: x = [2.79218309 2.78524251]  |  f(x) = 4107.596032235283\n",
      "Newton method: x = [0.10933726 1.02338246]  |  f(x) = 14.275382722231637\n",
      "BFGS: x = [nan nan]  |  f(x) = nan\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Starting point: [1 1], number of steps: 100\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Gradient Descent: x = [1.49509168 0.20055461]  |  f(x) = 2.0612658562698885\n",
      "Polyak Gradient Descent: x = [1.80564831 0.10168477]  |  f(x) = 0.9037350154608734\n",
      "Nesterov Gradient Descent: x = [1.80570114 0.10466524]  |  f(x) = 0.9036382000265031\n",
      "AdaGrad Gradient Descent: x = [2.99018095 0.49749714]  |  f(x) = 1.5678469242125484e-05\n",
      "Newton method converged in 1 steps!\n",
      "Newton method: x = [0. 1.]  |  f(x) = 14.203125\n",
      "BFGS converged in 25 steps!\n",
      "BFGS: x = [3.  0.5]  |  f(x) = 5.127595883936577e-30\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Starting point: [4.5 4.5], number of steps: 100\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Gradient Descent: x = [nan nan]  |  f(x) = nan\n",
      "Polyak Gradient Descent: x = [nan nan]  |  f(x) = nan\n",
      "Nesterov Gradient Descent: x = [nan nan]  |  f(x) = nan\n",
      "AdaGrad Gradient Descent: x = [1.99408486 1.9412833 ]  |  f(x) = 303.4195630388324\n",
      "Newton method converged in 12 steps!\n",
      "Newton method: x = [8.63454206e-13 1.00000000e+00]  |  f(x) = 14.203125\n",
      "BFGS: x = [nan nan]  |  f(x) = nan\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\matej\\AppData\\Local\\Temp\\ipykernel_14536\\1666910542.py:33: RuntimeWarning: overflow encountered in scalar multiply\n",
      "  dx = 2*(1.5 - x + x*y)*(y-1) + 2*(2.25 - x + x*(y ** 2))*(y ** 2 - 1) + 2*(2.625 - x + x*(y ** 3))*(y ** 3 -1)\n",
      "C:\\Users\\matej\\AppData\\Local\\Temp\\ipykernel_14536\\1666910542.py:34: RuntimeWarning: overflow encountered in scalar multiply\n",
      "  dy = 2*x*(1.5 - x + x*y) + 4*x*y*(2.25 - x + x*(y ** 2)) + 6*x*(y ** 2)*(2.625 - x + x*(y ** 3))\n",
      "C:\\Users\\matej\\AppData\\Local\\Temp\\ipykernel_14536\\1666910542.py:33: RuntimeWarning: invalid value encountered in scalar add\n",
      "  dx = 2*(1.5 - x + x*y)*(y-1) + 2*(2.25 - x + x*(y ** 2))*(y ** 2 - 1) + 2*(2.625 - x + x*(y ** 3))*(y ** 3 -1)\n",
      "C:\\Users\\matej\\AppData\\Local\\Temp\\ipykernel_14536\\1666910542.py:34: RuntimeWarning: invalid value encountered in scalar add\n",
      "  dy = 2*x*(1.5 - x + x*y) + 4*x*y*(2.25 - x + x*(y ** 2)) + 6*x*(y ** 2)*(2.625 - x + x*(y ** 3))\n",
      "C:\\Users\\matej\\AppData\\Local\\Temp\\ipykernel_14536\\2264529320.py:21: RuntimeWarning: invalid value encountered in matmul\n",
      "  nom1 = (np.outer(delta_k, gamma_k) @ H_approx) + (H_approx @ np.outer(gamma_k, delta_k))\n",
      "C:\\Users\\matej\\AppData\\Local\\Temp\\ipykernel_14536\\2264529320.py:24: RuntimeWarning: invalid value encountered in matmul\n",
      "  nom2 = np.inner(gamma_k, H_approx@gamma_k)\n"
     ]
    }
   ],
   "source": [
    "starting_points3 = [np.array([1, 1]), np.array([4.5, 4.5])]\n",
    "\n",
    "for step, starting_point in product(steps, starting_points3):\n",
    "\n",
    "    print(f\"Starting point: {starting_point}, number of steps: {step}\")\n",
    "    print(\"-\"*100)\n",
    "    compare_methods_with_steps(starting_point, f3, grad_f3, hessian_f3, step, 1e-10, 0.001)\n",
    "    print(\"-\"*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "65121339",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_methods_with_time(starting_point, function, gradient, hessian, time, epsilon):\n",
    "\n",
    "    point1, points1 = timed_gradient_descent(starting_point, 0.001, gradient, time_limit=time, epsilon=epsilon)\n",
    "    print(f\"Gradient Descent: x = {point1}  |  f(x) = {function(*point1)}\")\n",
    "\n",
    "    point2, points2 = timed_polyak_gradient_descent(starting_point, 0.001, 0.5, gradient, time_limit=time, epsilon=epsilon)\n",
    "    print(f\"Polyak Gradient Descent: x = {point2}  |  f(x) = {function(*point2)}\")\n",
    "\n",
    "    point3, points3 = timed_nesterov_gradient_descent(starting_point, 0.001, 0.5, gradient, time_limit=time, epsilon=epsilon)\n",
    "    print(f\"Nesterov Gradient Descent: x = {point3}  |  f(x) = {function(*point3)}\")\n",
    "\n",
    "    point4, points4 = timed_adaGrad(starting_point, 1, gradient, time_limit=time, epsilon=epsilon)\n",
    "    print(f\"AdaGrad Gradient Descent: x = {point4}  |  f(x) = {function(*point4)}\")\n",
    "\n",
    "    point5, points5 = timed_newton_method(starting_point, gradient, hessian, time_limit=time, epsilon=epsilon)\n",
    "    print(f\"Newton method: x = {point5}  |  f(x) = {function(*point5)}\")\n",
    "\n",
    "    point6, points6 = timed_BFGS(starting_point, gradient, time_limit=time, epsilon=epsilon)\n",
    "    print(f\"BFGS: x = {point6}  |  f(x) = {function(*point6)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ed3c2074",
   "metadata": {},
   "outputs": [],
   "source": [
    "times = [0.1, 1, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f51cb3ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting point: [0 0 0], time limit: 0.1s\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Gradient descent converged in 0.054511070251464844 seconds!\n",
      "Gradient Descent: x = [-0.16666665 -0.22916665  0.16666664]  |  f(x) = -0.197916666666665\n",
      "Polyak gradient descent converged in 0.024447917938232422 steps!\n",
      "Polyak Gradient Descent: x = [-0.16666666 -0.22916666  0.16666665]  |  f(x) = -0.1979166666666662\n",
      "Nesterov gradient descent converged in 0.029648303985595703 seconds!\n",
      "Nesterov Gradient Descent: x = [-0.16666666 -0.22916666  0.16666665]  |  f(x) = -0.19791666666666627\n",
      "AdaGrad converged in 0.0019998550415039062 seconds!\n",
      "AdaGrad Gradient Descent: x = [-0.16666667 -0.22916667  0.16666667]  |  f(x) = -0.19791666666666666\n",
      "Newton method converged in 0.0 seconds!\n",
      "Newton method: x = [-0.16666667 -0.22916667  0.16666667]  |  f(x) = -0.19791666666666663\n",
      "BFGS converged in 0.0 seconds!\n",
      "BFGS: x = [-0.16666667 -0.22916667  0.16666667]  |  f(x) = -0.19791666666666666\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Starting point: [1 1 0], time limit: 0.1s\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Gradient descent converged in 0.04077553749084473 seconds!\n",
      "Gradient Descent: x = [-0.16666665 -0.22916665  0.16666664]  |  f(x) = -0.19791666666666496\n",
      "Polyak gradient descent converged in 0.02591705322265625 steps!\n",
      "Polyak Gradient Descent: x = [-0.16666666 -0.22916666  0.16666665]  |  f(x) = -0.19791666666666627\n",
      "Nesterov gradient descent converged in 0.034459829330444336 seconds!\n",
      "Nesterov Gradient Descent: x = [-0.16666666 -0.22916666  0.16666665]  |  f(x) = -0.19791666666666627\n",
      "AdaGrad converged in 0.002946615219116211 seconds!\n",
      "AdaGrad Gradient Descent: x = [-0.16666667 -0.22916667  0.16666667]  |  f(x) = -0.19791666666666666\n",
      "Newton method converged in 0.0 seconds!\n",
      "Newton method: x = [-0.16666667 -0.22916667  0.16666667]  |  f(x) = -0.19791666666666669\n",
      "BFGS converged in 0.0009989738464355469 seconds!\n",
      "BFGS: x = [-0.16666667 -0.22916667  0.16666667]  |  f(x) = -0.19791666666666666\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Starting point: [0 0 0], time limit: 1s\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Gradient descent converged in 0.05116868019104004 seconds!\n",
      "Gradient Descent: x = [-0.16666665 -0.22916665  0.16666664]  |  f(x) = -0.197916666666665\n",
      "Polyak gradient descent converged in 0.02422952651977539 steps!\n",
      "Polyak Gradient Descent: x = [-0.16666666 -0.22916666  0.16666665]  |  f(x) = -0.1979166666666662\n",
      "Nesterov gradient descent converged in 0.03098011016845703 seconds!\n",
      "Nesterov Gradient Descent: x = [-0.16666666 -0.22916666  0.16666665]  |  f(x) = -0.19791666666666627\n",
      "AdaGrad converged in 0.0020194053649902344 seconds!\n",
      "AdaGrad Gradient Descent: x = [-0.16666667 -0.22916667  0.16666667]  |  f(x) = -0.19791666666666666\n",
      "Newton method converged in 0.0 seconds!\n",
      "Newton method: x = [-0.16666667 -0.22916667  0.16666667]  |  f(x) = -0.19791666666666663\n",
      "BFGS converged in 0.000997781753540039 seconds!\n",
      "BFGS: x = [-0.16666667 -0.22916667  0.16666667]  |  f(x) = -0.19791666666666666\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Starting point: [1 1 0], time limit: 1s\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Gradient descent converged in 0.0395355224609375 seconds!\n",
      "Gradient Descent: x = [-0.16666665 -0.22916665  0.16666664]  |  f(x) = -0.19791666666666496\n",
      "Polyak gradient descent converged in 0.031096696853637695 steps!\n",
      "Polyak Gradient Descent: x = [-0.16666666 -0.22916666  0.16666665]  |  f(x) = -0.19791666666666627\n",
      "Nesterov gradient descent converged in 0.03721499443054199 seconds!\n",
      "Nesterov Gradient Descent: x = [-0.16666666 -0.22916666  0.16666665]  |  f(x) = -0.19791666666666627\n",
      "AdaGrad converged in 0.0010018348693847656 seconds!\n",
      "AdaGrad Gradient Descent: x = [-0.16666667 -0.22916667  0.16666667]  |  f(x) = -0.19791666666666666\n",
      "Newton method converged in 0.0 seconds!\n",
      "Newton method: x = [-0.16666667 -0.22916667  0.16666667]  |  f(x) = -0.19791666666666669\n",
      "BFGS converged in 0.0 seconds!\n",
      "BFGS: x = [-0.16666667 -0.22916667  0.16666667]  |  f(x) = -0.19791666666666666\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Starting point: [0 0 0], time limit: 2s\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Gradient descent converged in 0.04417061805725098 seconds!\n",
      "Gradient Descent: x = [-0.16666665 -0.22916665  0.16666664]  |  f(x) = -0.197916666666665\n",
      "Polyak gradient descent converged in 0.03071451187133789 steps!\n",
      "Polyak Gradient Descent: x = [-0.16666666 -0.22916666  0.16666665]  |  f(x) = -0.1979166666666662\n",
      "Nesterov gradient descent converged in 0.03223276138305664 seconds!\n",
      "Nesterov Gradient Descent: x = [-0.16666666 -0.22916666  0.16666665]  |  f(x) = -0.19791666666666627\n",
      "AdaGrad converged in 0.002001047134399414 seconds!\n",
      "AdaGrad Gradient Descent: x = [-0.16666667 -0.22916667  0.16666667]  |  f(x) = -0.19791666666666666\n",
      "Newton method converged in 0.0 seconds!\n",
      "Newton method: x = [-0.16666667 -0.22916667  0.16666667]  |  f(x) = -0.19791666666666663\n",
      "BFGS converged in 0.00099945068359375 seconds!\n",
      "BFGS: x = [-0.16666667 -0.22916667  0.16666667]  |  f(x) = -0.19791666666666666\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Starting point: [1 1 0], time limit: 2s\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Gradient descent converged in 0.04777336120605469 seconds!\n",
      "Gradient Descent: x = [-0.16666665 -0.22916665  0.16666664]  |  f(x) = -0.19791666666666496\n",
      "Polyak gradient descent converged in 0.026877641677856445 steps!\n",
      "Polyak Gradient Descent: x = [-0.16666666 -0.22916666  0.16666665]  |  f(x) = -0.19791666666666627\n",
      "Nesterov gradient descent converged in 0.03244638442993164 seconds!\n",
      "Nesterov Gradient Descent: x = [-0.16666666 -0.22916666  0.16666665]  |  f(x) = -0.19791666666666627\n",
      "AdaGrad converged in 0.001995563507080078 seconds!\n",
      "AdaGrad Gradient Descent: x = [-0.16666667 -0.22916667  0.16666667]  |  f(x) = -0.19791666666666666\n",
      "Newton method converged in 0.0 seconds!\n",
      "Newton method: x = [-0.16666667 -0.22916667  0.16666667]  |  f(x) = -0.19791666666666669\n",
      "BFGS converged in 0.0 seconds!\n",
      "BFGS: x = [-0.16666667 -0.22916667  0.16666667]  |  f(x) = -0.19791666666666666\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "starting_points1 = [np.array([0,0,0]), np.array([1,1,0])]\n",
    "\n",
    "for t, starting_point in product(times, starting_points1):\n",
    "\n",
    "    print(f\"Starting point: {starting_point}, time limit: {t}s\")\n",
    "    print(\"-\"*100)\n",
    "    compare_methods_with_time(starting_point, f1, grad_f1, hessian_f1, t, 1e-10)\n",
    "    print(\"-\"*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9aa15674",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting point: [1.2 1.2 1.2], time limit: 0.1\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Gradient Descent: x = [1.00056081 1.00112408 1.00225478]  |  f(x) = 1.5813911125107483e-06\n",
      "Polyak Gradient Descent: x = [1.00000255 1.00000512 1.00001026]  |  f(x) = 3.278509400543577e-11\n",
      "Nesterov Gradient Descent: x = [1.00001083 1.00002171 1.00004352]  |  f(x) = 5.897056040344255e-10\n",
      "AdaGrad Gradient Descent: x = [0.99694597 0.99273154 0.98644754]  |  f(x) = 0.0002857789637805224\n",
      "Newton method converged in 0.0 seconds!\n",
      "Newton method: x = [1. 1. 1.]  |  f(x) = 1.6787946139234657e-28\n",
      "BFGS converged in 0.0 seconds!\n",
      "BFGS: x = [1. 1. 1.]  |  f(x) = 3.0297189141144485e-28\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Starting point: [-1.   1.2  1.2], time limit: 0.1\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Gradient Descent: x = [0.99800376 0.99600389 0.99200468]  |  f(x) = 1.9996035245054233e-05\n",
      "Polyak Gradient Descent: x = [0.99995014 0.99990008 0.9997997 ]  |  f(x) = 1.2496254874456082e-08\n",
      "Nesterov Gradient Descent: x = [0.99950349 0.99900534 0.99800694]  |  f(x) = 1.2384637225226998e-06\n",
      "AdaGrad Gradient Descent: x = [0.97500773 0.95066048 0.9036004 ]  |  f(x) = 0.0030614445130465375\n",
      "Newton method converged in 0.0 seconds!\n",
      "Newton method: x = [1. 1. 1.]  |  f(x) = 1.5407439555097887e-30\n",
      "BFGS converged in 0.005018472671508789 seconds!\n",
      "BFGS: x = [1. 1. 1.]  |  f(x) = 6.319268888886068e-27\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Starting point: [1.2 1.2 1.2], time limit: 1\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Gradient descent converged in 0.2709226608276367 seconds!\n",
      "Gradient Descent: x = [1.00000005 1.00000009 1.00000018]  |  f(x) = 1.0505809944749373e-14\n",
      "Polyak gradient descent converged in 0.1717822551727295 steps!\n",
      "Polyak Gradient Descent: x = [1.00000002 1.00000005 1.00000009]  |  f(x) = 2.6174702944447187e-15\n",
      "Nesterov gradient descent converged in 0.20466160774230957 seconds!\n",
      "Nesterov Gradient Descent: x = [1.00000002 1.00000005 1.00000009]  |  f(x) = 2.6184968830468743e-15\n",
      "AdaGrad converged in 0.4131896495819092 seconds!\n",
      "AdaGrad Gradient Descent: x = [0.99999997 0.99999995 0.99999989]  |  f(x) = 3.773289624116732e-15\n",
      "Newton method converged in 0.0 seconds!\n",
      "Newton method: x = [1. 1. 1.]  |  f(x) = 1.6787946139234657e-28\n",
      "BFGS converged in 0.0026607513427734375 seconds!\n",
      "BFGS: x = [1. 1. 1.]  |  f(x) = 3.0297189141144485e-28\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Starting point: [-1.   1.2  1.2], time limit: 1\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Gradient descent converged in 0.28811216354370117 seconds!\n",
      "Gradient Descent: x = [0.99999995 0.99999991 0.99999982]  |  f(x) = 1.0506246200056892e-14\n",
      "Polyak gradient descent converged in 0.18489289283752441 steps!\n",
      "Polyak Gradient Descent: x = [0.99999998 0.99999995 0.99999991]  |  f(x) = 2.6179984680286644e-15\n",
      "Nesterov gradient descent converged in 0.23955893516540527 seconds!\n",
      "Nesterov Gradient Descent: x = [0.99999998 0.99999995 0.99999991]  |  f(x) = 2.6182950435492194e-15\n",
      "AdaGrad converged in 0.4301912784576416 seconds!\n",
      "AdaGrad Gradient Descent: x = [0.99999997 0.99999994 0.99999988]  |  f(x) = 4.6156067699267526e-15\n",
      "Newton method converged in 0.0 seconds!\n",
      "Newton method: x = [1. 1. 1.]  |  f(x) = 1.5407439555097887e-30\n",
      "BFGS converged in 0.006009101867675781 seconds!\n",
      "BFGS: x = [1. 1. 1.]  |  f(x) = 6.319268888886068e-27\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Starting point: [1.2 1.2 1.2], time limit: 2\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Gradient descent converged in 0.2562096118927002 seconds!\n",
      "Gradient Descent: x = [1.00000005 1.00000009 1.00000018]  |  f(x) = 1.0505809944749373e-14\n",
      "Polyak gradient descent converged in 0.1603085994720459 steps!\n",
      "Polyak Gradient Descent: x = [1.00000002 1.00000005 1.00000009]  |  f(x) = 2.6174702944447187e-15\n",
      "Nesterov gradient descent converged in 0.23155665397644043 seconds!\n",
      "Nesterov Gradient Descent: x = [1.00000002 1.00000005 1.00000009]  |  f(x) = 2.6184968830468743e-15\n",
      "AdaGrad converged in 0.7108559608459473 seconds!\n",
      "AdaGrad Gradient Descent: x = [0.99999997 0.99999995 0.99999989]  |  f(x) = 3.773289624116732e-15\n",
      "Newton method converged in 0.0 seconds!\n",
      "Newton method: x = [1. 1. 1.]  |  f(x) = 1.6787946139234657e-28\n",
      "BFGS converged in 0.0019996166229248047 seconds!\n",
      "BFGS: x = [1. 1. 1.]  |  f(x) = 3.0297189141144485e-28\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Starting point: [-1.   1.2  1.2], time limit: 2\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Gradient descent converged in 0.49159812927246094 seconds!\n",
      "Gradient Descent: x = [0.99999995 0.99999991 0.99999982]  |  f(x) = 1.0506246200056892e-14\n",
      "Polyak gradient descent converged in 0.37106966972351074 steps!\n",
      "Polyak Gradient Descent: x = [0.99999998 0.99999995 0.99999991]  |  f(x) = 2.6179984680286644e-15\n",
      "Nesterov gradient descent converged in 0.39603400230407715 seconds!\n",
      "Nesterov Gradient Descent: x = [0.99999998 0.99999995 0.99999991]  |  f(x) = 2.6182950435492194e-15\n",
      "AdaGrad converged in 0.7836406230926514 seconds!\n",
      "AdaGrad Gradient Descent: x = [0.99999997 0.99999994 0.99999988]  |  f(x) = 4.6156067699267526e-15\n",
      "Newton method converged in 0.0 seconds!\n",
      "Newton method: x = [1. 1. 1.]  |  f(x) = 1.5407439555097887e-30\n",
      "BFGS converged in 0.012581825256347656 seconds!\n",
      "BFGS: x = [1. 1. 1.]  |  f(x) = 6.319268888886068e-27\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "starting_points2 = [np.array([1.2, 1.2, 1.2]), np.array([-1, 1.2, 1.2])]\n",
    "\n",
    "for t, starting_point in product(times, starting_points2):\n",
    "\n",
    "    print(f\"Starting point: {starting_point}, time limit: {t}\")\n",
    "    print(\"-\"*100)\n",
    "    compare_methods_with_time(starting_point, f2, grad_f2, hessian_f2, t, 1e-10)\n",
    "    print(\"-\"*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2eccb397",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting point: [1 1], time limit: 0.1\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Gradient Descent: x = [2.93914252 0.48440009]  |  f(x) = 0.0006350605341245992\n",
      "Polyak Gradient Descent: x = [2.96429967 0.49094917]  |  f(x) = 0.00021240176885528264\n",
      "Nesterov Gradient Descent: x = [2.93647354 0.48369684]  |  f(x) = 0.0006940900958730335\n",
      "AdaGrad Gradient Descent: x = [3.00000592 0.49999426]  |  f(x) = 1.204757807588679e-09\n",
      "Newton method converged in 0.0 seconds!\n",
      "Newton method: x = [0. 1.]  |  f(x) = 14.203125\n",
      "BFGS converged in 0.0020012855529785156 seconds!\n",
      "BFGS: x = [3.  0.5]  |  f(x) = 5.127595883936577e-30\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Starting point: [4.5 4.5], time limit: 0.1\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Gradient Descent: x = [nan nan]  |  f(x) = nan\n",
      "Polyak Gradient Descent: x = [nan nan]  |  f(x) = nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\matej\\AppData\\Local\\Temp\\ipykernel_14536\\1666910542.py:33: RuntimeWarning: overflow encountered in scalar multiply\n",
      "  dx = 2*(1.5 - x + x*y)*(y-1) + 2*(2.25 - x + x*(y ** 2))*(y ** 2 - 1) + 2*(2.625 - x + x*(y ** 3))*(y ** 3 -1)\n",
      "C:\\Users\\matej\\AppData\\Local\\Temp\\ipykernel_14536\\1666910542.py:34: RuntimeWarning: overflow encountered in scalar multiply\n",
      "  dy = 2*x*(1.5 - x + x*y) + 4*x*y*(2.25 - x + x*(y ** 2)) + 6*x*(y ** 2)*(2.625 - x + x*(y ** 3))\n",
      "C:\\Users\\matej\\AppData\\Local\\Temp\\ipykernel_14536\\1666910542.py:33: RuntimeWarning: invalid value encountered in scalar add\n",
      "  dx = 2*(1.5 - x + x*y)*(y-1) + 2*(2.25 - x + x*(y ** 2))*(y ** 2 - 1) + 2*(2.625 - x + x*(y ** 3))*(y ** 3 -1)\n",
      "C:\\Users\\matej\\AppData\\Local\\Temp\\ipykernel_14536\\1666910542.py:34: RuntimeWarning: invalid value encountered in scalar add\n",
      "  dy = 2*x*(1.5 - x + x*y) + 4*x*y*(2.25 - x + x*(y ** 2)) + 6*x*(y ** 2)*(2.625 - x + x*(y ** 3))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nesterov Gradient Descent: x = [nan nan]  |  f(x) = nan\n",
      "AdaGrad Gradient Descent: x = [1.44587407 0.96376539]  |  f(x) = 12.823659784560629\n",
      "Newton method converged in 0.0 seconds!\n",
      "Newton method: x = [3.37253814e-26 1.00000000e+00]  |  f(x) = 14.203125\n",
      "BFGS: x = [nan nan]  |  f(x) = nan\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Starting point: [1 1], time limit: 1\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\matej\\AppData\\Local\\Temp\\ipykernel_14536\\1854726025.py:22: RuntimeWarning: invalid value encountered in matmul\n",
      "  nom1 = (np.outer(delta_k, gamma_k) @ H_approx) + (H_approx @ np.outer(gamma_k, delta_k))\n",
      "C:\\Users\\matej\\AppData\\Local\\Temp\\ipykernel_14536\\1854726025.py:25: RuntimeWarning: invalid value encountered in matmul\n",
      "  nom2 = np.inner(gamma_k, H_approx@gamma_k)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient descent converged in 0.7471287250518799 seconds!\n",
      "Gradient Descent: x = [2.99999968 0.49999992]  |  f(x) = 1.6569564286332804e-14\n",
      "Polyak gradient descent converged in 0.506929874420166 steps!\n",
      "Polyak Gradient Descent: x = [2.99999984 0.49999996]  |  f(x) = 4.1337000146402385e-15\n",
      "Nesterov gradient descent converged in 0.6416788101196289 seconds!\n",
      "Nesterov Gradient Descent: x = [2.99999984 0.49999996]  |  f(x) = 4.136092788266923e-15\n",
      "AdaGrad converged in 0.17331552505493164 seconds!\n",
      "AdaGrad Gradient Descent: x = [3.  0.5]  |  f(x) = 4.369779070677727e-20\n",
      "Newton method converged in 0.0 seconds!\n",
      "Newton method: x = [0. 1.]  |  f(x) = 14.203125\n",
      "BFGS converged in 0.0009963512420654297 seconds!\n",
      "BFGS: x = [3.  0.5]  |  f(x) = 5.127595883936577e-30\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Starting point: [4.5 4.5], time limit: 1\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Gradient Descent: x = [nan nan]  |  f(x) = nan\n",
      "Polyak Gradient Descent: x = [nan nan]  |  f(x) = nan\n",
      "Nesterov Gradient Descent: x = [nan nan]  |  f(x) = nan\n",
      "AdaGrad Gradient Descent: x = [2.28040138 0.27923713]  |  f(x) = 0.1977916806039413\n",
      "Newton method converged in 0.0 seconds!\n",
      "Newton method: x = [3.37253814e-26 1.00000000e+00]  |  f(x) = 14.203125\n",
      "BFGS: x = [nan nan]  |  f(x) = nan\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Starting point: [1 1], time limit: 2\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Gradient descent converged in 0.7973470687866211 seconds!\n",
      "Gradient Descent: x = [2.99999968 0.49999992]  |  f(x) = 1.6569564286332804e-14\n",
      "Polyak gradient descent converged in 0.5482883453369141 steps!\n",
      "Polyak Gradient Descent: x = [2.99999984 0.49999996]  |  f(x) = 4.1337000146402385e-15\n",
      "Nesterov gradient descent converged in 0.7141554355621338 seconds!\n",
      "Nesterov Gradient Descent: x = [2.99999984 0.49999996]  |  f(x) = 4.136092788266923e-15\n",
      "AdaGrad converged in 0.1456165313720703 seconds!\n",
      "AdaGrad Gradient Descent: x = [3.  0.5]  |  f(x) = 4.369779070677727e-20\n",
      "Newton method converged in 0.0 seconds!\n",
      "Newton method: x = [0. 1.]  |  f(x) = 14.203125\n",
      "BFGS converged in 0.0020062923431396484 seconds!\n",
      "BFGS: x = [3.  0.5]  |  f(x) = 5.127595883936577e-30\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Starting point: [4.5 4.5], time limit: 2\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Gradient Descent: x = [nan nan]  |  f(x) = nan\n",
      "Polyak Gradient Descent: x = [nan nan]  |  f(x) = nan\n",
      "Nesterov Gradient Descent: x = [nan nan]  |  f(x) = nan\n",
      "AdaGrad Gradient Descent: x = [2.45511859 0.31634177]  |  f(x) = 0.09480192917790639\n",
      "Newton method converged in 0.0 seconds!\n",
      "Newton method: x = [3.37253814e-26 1.00000000e+00]  |  f(x) = 14.203125\n",
      "BFGS: x = [nan nan]  |  f(x) = nan\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "starting_points3 = [np.array([1, 1]), np.array([4.5, 4.5])]\n",
    "\n",
    "for t, starting_point in product(times, starting_points3):\n",
    "\n",
    "    print(f\"Starting point: {starting_point}, time limit: {t}\")\n",
    "    print(\"-\"*100)\n",
    "    compare_methods_with_time(starting_point, f3, grad_f3, hessian_f3, t, 1e-10)\n",
    "    print(\"-\"*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b0736d3",
   "metadata": {},
   "source": [
    "# Linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "14464141",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generating_process(num):\n",
    "    np.random.seed(42)\n",
    "\n",
    "    xs = []\n",
    "    ys = []\n",
    "    for i in range(1, num):\n",
    "        \n",
    "        x = i\n",
    "        y = i + np.random.uniform(0, 1)\n",
    "\n",
    "        xs.append(x)\n",
    "        ys.append(y)\n",
    "    return np.array(xs), np.array(ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0691732f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class linear_regression():\n",
    "\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def gradient(self, betas):\n",
    "        residuals = self.y - np.dot(betas, self.X)\n",
    "\n",
    "        return -2 * np.dot(self.X.T, residuals)\n",
    "\n",
    "    def build(self):\n",
    "        self.X = np.column_stack((self.X, np.ones(self.X.shape[0])))\n",
    "\n",
    "        betas, _ = gradient_descent(np.array([0,0]), 0.1, self.gradient, 100)\n",
    "        return betas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "91a52d71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size: 50\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Gradient descent converged in 9 steps!\n",
      "Gradient descent: [1.01255084 0.03153748]\n",
      "Stochastic gradient descent converged in 19 steps!\n",
      "Stochastic gradient descent: [1.01324697 0.0318547 ]\n",
      "Newton method converged in 1 steps!\n",
      "Newton method: [0.99937674 0.46683347]\n",
      "BFGS converged in 4 steps!\n",
      "BFGS: [0.99937674 0.46683347]\n",
      "L-FBGS converged in 4 steps\n",
      "L-BFGS: [0.99937674 0.46683347]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Data size: 100\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Gradient descent converged in 9 steps!\n",
      "Gradient descent: [1.00700063 0.0153971 ]\n",
      "Stochastic gradient descent converged in 15 steps!\n",
      "Stochastic gradient descent: [1.01291836 0.0187166 ]\n",
      "Newton method converged in 1 steps!\n",
      "Newton method: [1.00043738 0.45197123]\n",
      "BFGS converged in 4 steps!\n",
      "BFGS: [1.00043738 0.45197123]\n",
      "L-FBGS converged in 2 steps\n",
      "L-BFGS: [1.00702116 0.0152796 ]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Data size: 1000\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Gradient descent converged in 9 steps!\n",
      "Gradient descent: [1.00071031 0.00150432]\n",
      "Stochastic gradient descent converged in 6 steps!\n",
      "Stochastic gradient descent: [1.00171715 0.00133734]\n",
      "Newton method converged in 1 steps!\n",
      "Newton method: [0.99997542 0.5025933 ]\n",
      "BFGS converged in 4 steps!\n",
      "BFGS: [0.99997542 0.5025933 ]\n",
      "L-FBGS converged in 2 steps\n",
      "L-BFGS: [1.00072743 0.00150297]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Data size: 10000\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Gradient descent converged in 9 steps!\n",
      "Gradient descent: [1.00005688e+00 1.50041005e-04]\n",
      "Stochastic gradient descent converged in 11 steps!\n",
      "Stochastic gradient descent: [1.00332232e+00 2.38865261e-04]\n",
      "Newton method converged in 1 steps!\n",
      "Newton method: [0.99999891 0.49962696]\n",
      "BFGS converged in 15 steps!\n",
      "BFGS: [0.99999891 0.49962681]\n",
      "L-FBGS converged in 2 steps\n",
      "L-BFGS: [1.00007384e+00 1.50029817e-04]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Data size: 100000\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Gradient descent converged in 9 steps!\n",
      "Gradient descent: [9.99990553e-01 1.50001832e-05]\n",
      "Stochastic gradient descent converged in 5 steps!\n",
      "Stochastic gradient descent: [1.00130147e+00 5.89528911e-06]\n",
      "Newton method converged in 1 steps!\n",
      "Newton method: [0.99999999 0.49984935]\n",
      "BFGS converged in 5 steps!\n",
      "BFGS: [0.99999999 0.49984935]\n",
      "L-FBGS converged in 2 steps\n",
      "L-BFGS: [1.00000749e+00 1.50003094e-05]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Data size: 1000000\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Gradient descent converged in 9 steps!\n",
      "Gradient descent: [9.99983815e-01 1.49997897e-06]\n",
      "Stochastic gradient descent converged in 9 steps!\n",
      "Stochastic gradient descent: [1.00022972e+00 1.61790802e-06]\n",
      "Newton method converged in 1 steps!\n",
      "Newton method: [1.         0.50001873]\n",
      "BFGS converged in 5 steps!\n",
      "BFGS: [1.         0.50001873]\n",
      "L-FBGS converged in 2 steps\n",
      "L-BFGS: [1.00000075e+00 1.50024575e-06]\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "global xs\n",
    "global ys\n",
    "\n",
    "starting_point = np.array([0,0])\n",
    "n_iters = 1000\n",
    "epsilon = 0.0001\n",
    "mu = 0.5\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "def gradient(b1, b0):\n",
    "    betas = np.array([b1,b0])\n",
    "    residuals = ys - np.dot(xs, betas)\n",
    "    return (-2/xs.shape[0]) * np.dot(xs.T, residuals) # i used MSE instead of just a sum of errors for numerical stability\n",
    "\n",
    "def stochastic_gradient(b1, b0):\n",
    "    ind = random.randint(0, xs.shape[0]-1)\n",
    "    betas = np.array([b1,b0])\n",
    "    x_cur = xs[ind]\n",
    "    y_cur = ys[ind]\n",
    "    residual = y_cur - np.dot(x_cur, betas)\n",
    "    return -2*np.dot(x_cur.T, residual)\n",
    "\n",
    "def hessian(b1, b0):\n",
    "    return (2/xs.shape[0]) * np.dot(xs.T, xs)\n",
    "\n",
    "data_sizes = [50, 100, 1000, 10000, 100000, 1000000]\n",
    "for size in data_sizes:\n",
    "    xs, ys = data_generating_process(size)\n",
    "\n",
    "    print(f\"Data size: {size}\")\n",
    "    print(\"-\"*100)\n",
    "    xs = np.column_stack((xs, np.ones(xs.shape[0])))\n",
    "\n",
    "    betas1, _ = gradient_descent(starting_point, 1/size**2, gradient, n_iters, epsilon)\n",
    "    print(f\"Gradient descent: {betas1}\")\n",
    "    betas2, _ = stochastic_gradient_descent(starting_point, 1/size**2, stochastic_gradient, n_iters, epsilon)\n",
    "    print(f\"Stochastic gradient descent: {betas2}\")\n",
    "    betas3, _ = newton_method(starting_point, gradient, hessian, n_iters, epsilon)\n",
    "    print(f\"Newton method: {betas3}\")\n",
    "    betas4, _ = BFGS(starting_point, gradient, n_iters, epsilon)\n",
    "    print(f\"BFGS: {betas4}\")\n",
    "    betas5, _ = L_BFGS(starting_point, gradient, 10, n_iters, epsilon)\n",
    "    print(f\"L-BFGS: {betas5}\")\n",
    "    print(\"-\"*100)\n",
    "\n",
    "\n",
    "#b1, b0 = betas[0], betas[1]\n",
    "\n",
    "# x =  np.linspace(0, 10, 100)\n",
    "# y = b0 + b1 * x\n",
    "\n",
    "# plt.scatter(xs[:,0], ys)\n",
    "# plt.plot(x, y, label=f'y = {b0} + {b1}x', color='red')\n",
    "# print(betas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ac71e4b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.  , 27.75],\n",
       "       [27.75, 68.5 ]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hessian_f3(*np.array([1,1]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLDS_P2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
